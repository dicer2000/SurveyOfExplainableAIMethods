# Survey of Explainable AI Techniques for Vision Transformers
## Code for Study
--------

These experiements were written in 2023 to support research being done for the research paper "Survey of Explainable AI Techniques for Vision Transformers"

**Researcher: Brett Huffman**\
**Faculty Advisor: Badri Adhikari, PhD**

We compared a number of explainability techniques for vision transformers by comparing their use, effectivness and complexity.  A varied number of implementations were used to test these methods.  Google Colab was utilized whenever possible.  In which case, you will find the implementation within the Notebooks folder, saved as a Jupiter notebook (.ipynb file).

In cases where the complexity of the method precluded use of Google Colab, python implementations were built.  This is mainly for the Perturbation methods, such as Lime.  You will find these files within the Code folder.


## Folders / Files
-----

* Code
    * kaggle.json - used for lime analysis.  You will need your own kaggle creds
    * lime.py - Method for Lime
    * occ2.py - Method for studying occlusion analysis
    * occlusion_sensativity.py - Method for studying occlusion sensativity
* Images
    * elephant.jpg - Test file for elephants
* Notebooks
    * notebooks will go here
    * lots of them!

* .gitignore - You know what this does
* README.md - You are here now!



Â©2024 Brett Huffman